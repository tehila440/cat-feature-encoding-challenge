# cat-feature-encoding-challenge
This was my first independent Kaggle competition.  I focused on cleaning the data, feature selection, feature engineering.  Worked on feature engineering numerical,
ordinal, and nominal columns.  I tested a few Machine Learning models including:  XGBoost, Logistic Regression, and 
Random Forest Regressor.  I utilized hyperparameter search for model optimization. 

The link to the competition is below.
https://www.kaggle.com/c/cat-in-the-dat-ii/overview

The dataset contains only categorical features, and includes:
* binary features
* low- and high-cardinality nominal features
* low- and high-cardinality ordinal features
* (potentially) cyclical features

Submissions were evaluated on area under the ROC curve between 
the predicted probability and the observed target
